{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN97p/YqTQ8QsMVhqZqF7zd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Eci6W-3Yt0b","executionInfo":{"status":"ok","timestamp":1745000147629,"user_tz":-330,"elapsed":28277,"user":{"displayName":"Namit","userId":"08031843768376211005"}},"outputId":"bea583ea-e0ed-4459-c030-6057594962a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# STEP 2: Load processed sequence data\n","import pandas as pd\n","import os\n","\n","project_root = \"/content/drive/MyDrive/FraudBehaviorEmbeddings\"\n","input_path = os.path.join(project_root, \"data/processed/transformer_input.pkl\")\n","df = pd.read_pickle(input_path)#deserialize pickel\n","\n","# Convert to list format\n","sequences = list(df['padded_sequence'].values)\n","labels = list(df['label_encoded'].values)\n","# print(sequences)\n","print(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mmKf6vuEZU6j","executionInfo":{"status":"ok","timestamp":1745000454785,"user_tz":-330,"elapsed":438,"user":{"displayName":"Namit","userId":"08031843768376211005"}},"outputId":"322fcd26-6dd0-4d43-e5a3-dcf38ea638c6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n"]}]},{"cell_type":"markdown","source":["Install Hugging face transformer\n"],"metadata":{"id":"_24VAeG-ZrBu"}},{"cell_type":"code","source":["!pip install transformers -q"],"metadata":{"id":"urulAvhwZwUc","executionInfo":{"status":"ok","timestamp":1745000658276,"user_tz":-330,"elapsed":3015,"user":{"displayName":"Namit","userId":"08031843768376211005"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["PREPARE dataset\n"],"metadata":{"id":"k9tptwGdJTb-"}},{"cell_type":"code","source":["#STEP 4  Prepare datasets\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","class FraudDataset(Dataset):\n","    def __init__(self, sequences, labels):\n","        self.sequences = torch.tensor(sequences, dtype=torch.long)\n","        self.labels = torch.tensor(labels, dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.sequences[idx],\n","            'attention_mask': (self.sequences[idx] != 0).long(),  # padding mask\n","            'labels': self.labels[idx]\n","        }\n","\n","X_train, X_val, y_train, y_val = train_test_split(sequences, labels, test_size=0.2, stratify=labels)\n","\n","train_data = FraudDataset(X_train, y_train)\n","val_data = FraudDataset(X_val, y_val)\n","\n","train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=8)"],"metadata":{"id":"lnBuQxE5ZYiq","executionInfo":{"status":"ok","timestamp":1745004396460,"user_tz":-330,"elapsed":5,"user":{"displayName":"Namit","userId":"08031843768376211005"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["LOAD TRANSFORMER MODEL"],"metadata":{"id":"DiYIadr4Xojp"}},{"cell_type":"code","source":["#  STEP 5: Load DistilBERT Model\n","from transformers import DistilBertForSequenceClassification, DistilBertConfig\n","\n","num_labels = df['label_encoded'].nunique()\n","# print(num_labels)\n","\n","config = DistilBertConfig(\n"," vocab_size=30522,    # Number of tokens DistilBERT understands\n","    n_heads=8,           # Number of attention heads (parallel attention mechanisms)\n","    dim=512,             # Hidden size of each token representation (default for DistilBERT)\n","    hidden_dim=2048,     # Size of feed-forward layers inside the Transformer block\n","    n_layers=6,          # Number of Transformer layers\n","    # num_labels=num_labels  # Number of output classes for classification\n"," num_labels=3# this provided me error in the training phase\n","\n",")\n","\n","model = DistilBertForSequenceClassification(config)\n"],"metadata":{"id":"OnmLZ60hbaJc","executionInfo":{"status":"ok","timestamp":1745007596822,"user_tz":-330,"elapsed":615,"user":{"displayName":"Namit","userId":"08031843768376211005"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# STEP 6: Train the model\n","\n","from torch.optim import AdamW\n","from tqdm import tqdm\n","\n","# from transformers import AdamW\n","from tqdm import tqdm\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","EPOCHS = 3\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(train_loader):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    print(f\"ðŸ”¥ Epoch {epoch+1} - Loss: {total_loss / len(train_loader)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qhif_eDibciC","executionInfo":{"status":"ok","timestamp":1745007605937,"user_tz":-330,"elapsed":4140,"user":{"displayName":"Namit","userId":"08031843768376211005"}},"outputId":"77bb149c-f9dd-4b3b-cf44-fb903cb66970"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.48s/it]\n"]},{"output_type":"stream","name":"stdout","text":["ðŸ”¥ Epoch 1 - Loss: 1.3111671209335327\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["ðŸ”¥ Epoch 2 - Loss: 0.8585801124572754\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.29s/it]"]},{"output_type":"stream","name":"stdout","text":["ðŸ”¥ Epoch 3 - Loss: 0.6171958446502686\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# STEP 7: Save model\n","model_path = os.path.join(project_root, \"models/transformer/distilbert_fraud.pt\")\n","torch.save(model.state_dict(), model_path)\n","print(f\" Saved model to {model_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9OeaJbfdTzN","executionInfo":{"status":"ok","timestamp":1745007762204,"user_tz":-330,"elapsed":212,"user":{"displayName":"Namit","userId":"08031843768376211005"}},"outputId":"136811e5-f930-4fcc-b5c9-7c4bbb9ab9e4"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":[" Saved model to /content/drive/MyDrive/FraudBehaviorEmbeddings/models/transformer/distilbert_fraud.pt\n"]}]}]}